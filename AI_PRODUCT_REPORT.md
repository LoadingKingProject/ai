# AI PRODUCT REPORT - Air Mouse

---

## 1. 문제 정의 및 모델 선택 사유

### 1.1 해결하려는 문제
현대의 컴퓨팅 환경은 키보드와 마우스라는 물리적 입력 장치에 크게 의존하고 있습니다. 그러나 **발표(Presentation), 키오스크, 스마트 미러, 요리 중 레시피 확인** 등 물리적 장치 사용이 어렵거나 불편한 상황이 빈번히 발생합니다.
본 프로젝트는 **웹캠**이라는 보편적인 장비를 활용하여, 별도의 하드웨어 없이 **비접촉(Touchless) 제스처**만으로 컴퓨터를 제어할 수 있는 인터페이스를 제공함으로써 물리적 제약을 극복하고자 합니다.

### 1.2 선택 모델
*   **Face Analysis**: `MediaPipe Face Mesh` (468 랜드마크)
*   **Hand Tracking**: `MediaPipe Hands` (21 랜드마크)

### 1.3 선택 이유
1.  **온디바이스 실시간 처리 (Real-time On-device)**:
    *   MediaPipe는 경량화된 모델로, 고사양 GPU 없이 일반 CPU 환경에서도 **30fps 이상의 실시간 추론**이 가능합니다.
    *   서버로 영상을 보내지 않고 로컬에서 처리하므로 **프라이버시 문제**를 해결하고 **네트워크 지연(Latency)**을 최소화할 수 있습니다.
2.  **높은 정확도와 강건함**:
    *   손바닥 검출(Palm Detection) 후 랜드마크를 추적하는 2단계 파이프라인을 사용하여, 손이 가려지거나 조명이 변하는 환경에서도 안정적인 성능을 보입니다.
3.  **플랫폼 호환성**:
    *   Python(백엔드)뿐만 아니라 JS, Android, iOS 등 다양한 플랫폼을 지원하여 확장성이 뛰어납니다.

---

## 2. 데이터 파이프라인 및 전처리

### 2.1 INPUT / OUTPUT 설계

| 단계 | INPUT (입력) | PROCESS (처리) | OUTPUT (출력) |
|:---:|:---:|:---|:---:|
| **1** | **Webcam Feed**<br>(RGB Image, 640x480) | **이미지 전처리**<br>- 좌우 반전 (Mirroring)<br>- BGR to RGB 변환<br>- Base64 인코딩 (스트리밍용) | **Pre-processed Frame** |
| **2** | **Pre-processed Frame** | **AI 모델 추론 (MediaPipe)**<br>- Face Mesh (거리/비율)<br>- Hands (관절 좌표) | **Landmarks (x, y, z)**<br>(Normalized 0.0~1.0) |
| **3** | **Landmarks** | **로직 후처리 (Logic)**<br>- 좌표 스무딩 (Jitter 제거)<br>- 동적 임계값(Dynamic Threshold) 적용<br>- 제스처 판별 (Click, Drag, Zoom) | **Action Signal**<br>(Mouse Move, Click, Scroll)<br>**UI Data** (JSON) |

### 2.2 핵심 전처리 기술
*   **좌표 정규화 (Normalization)**: 화면 해상도에 종속되지 않도록 0~1 사이 값으로 변환하여 처리.
*   **이동 평균 필터 (Moving Average)**: 손 떨림 방지를 위해 이전 N개 프레임의 좌표를 평균 내어 부드러운 움직임 구현.
*   **동적 임계값 (Dynamic Threshold)**: 카메라와의 거리에 따라 손 크기가 변하는 문제를 해결하기 위해, 손바닥 크기에 비례하여 클릭/줌 인식 거리를 조절.

---

## 3. 성능 평가 및 한계점

### 3.1 평가 지표 : 인식 정확도 및 사용성
*   **Face Analysis**: 정면 얼굴 기준 거리 측정 오차 **±5cm 이내** (1m 기준).
*   **Hand Tracking**:
    *   **mAP (Mean Average Precision)**: 95.7% (MediaPipe 공식 벤치마크 기준).
    *   **FPS (Frames Per Second)**: i5 CPU 기준 평균 **28~32 fps** 유지.
*   **Latency**: 카메라 입력부터 마우스 동작까지 약 **50~80ms** 지연 (사용자가 위화감을 느끼지 않는 수준).

### 3.2 한계점
1.  **조명 의존성**: 매우 어둡거나 역광이 강한 환경에서는 인식률이 급격히 저하됨.
2.  **가려짐(Occlusion)**: 손이 얼굴을 가리거나, 다른 물체에 의해 손이 가려질 경우 트래킹이 끊길 수 있음.
3.  **제스처의 모호성**: '클릭' 동작(엄지-검지 맞닿음)과 단순한 손 움직임을 구분하는 데 있어 오작동 발생 가능성.

### 3.3 개선 방향 : 파인튜닝을 위한 데이터 예시
단순 거리 기반 로직의 한계를 넘기 위해, **제스처 분류 모델(Classifier)**을 추가 학습(Fine-tuning)시킬 수 있습니다.

*   **데이터셋 구축 예시**:
    *   **Class 0 (Idle)**: 손을 편 상태 이미지 1,000장.
    *   **Class 1 (Click)**: 엄지와 검지를 붙인 다양한 각도의 이미지 1,000장.
    *   **Class 2 (Fist)**: 주먹을 쥔 이미지 1,000장.
    *   **Class 3 (Victory)**: V 포즈 이미지 1,000장.
*   **활용**: 위 데이터로 경량화된 분류 모델(예: MobileNet)을 학습시켜 MediaPipe 랜드마크 데이터와 결합하면 오작동을 획기적으로 줄일 수 있습니다.

---

## 4. AI 프로덕트의 가치

### 4.1 적용 가능성
*   **PC용 (Desktop/Web)**:
    *   프레젠테이션 제어 도구 (PPT 넘기기, 포인터).
    *   접근성 보조 도구 (장애인 및 노약자를 위한 마우스 대체).
*   **모바일/키오스크**:
    *   비접촉 주문 시스템 (위생이 중요한 식당, 병원).
    *   요리 레시피 앱 (손에 물이 묻었을 때 스크롤/넘기기).

### 4.2 사용자 경험 (UX) 및 Pain Point 해결
*   **Pain Point**: "발표 중에 마우스를 잡으러 연단으로 돌아가야 한다", "요리 중에 스마트폰을 만지면 액정이 더러워진다".
*   **Solution**: **Air Mouse**는 공간의 제약 없이 허공에서의 제스처만으로 기기를 제어하게 하여, 사용자의 **행동 반경을 확장**하고 **작업의 흐름(Flow)이 끊기지 않도록** 돕습니다.
*   **VisionOS Style UI**: 단순한 기능 구현을 넘어, 미래지향적인 UI와 사운드 피드백(Access Granted 등)을 통해 사용자가 영화 속 주인공이 된 듯한 **몰입감(Immersion)**을 제공합니다.
